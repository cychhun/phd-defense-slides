\begin{frame}{Main Contributions and Findings (1/2)}
    \textbf{RQ1: How can we evaluate the quality of a story?}
    \begin{itemize}
        \item 6 criteria for human evaluation: \textbf{Relevance}, \textbf{Coherence}, \textbf{Empathy}, \textbf{Surprise}, \textbf{Engagement}, and \textbf{Complexity};
    \end{itemize}
    \textbf{RQ2: How good are existing models at generating stories?}
    \begin{itemize}
        \item \textbf{HANNA, a corpus containing 1,536 stories generated by 15 different systems} (1 human, 3 ASG-specific, 7 pretrained LMs, 4 LLMs);
        \item All non-LLM stories were rated \wrt\ our 6 criteria by 3 human annotators and 4 different LLMs with 4 different Eval-Prompts;
        \item \textbf{LLMs seem to perform as well as human writers for this specific setting}, according to Beluga-13B ratings.
    \end{itemize}
\end{frame}

\begin{frame}{Main Contributions and Findings (2/2)}
    \textbf{RQ3: To which extent can we use automatic measures for story evaluation?}
    \begin{itemize}
        \item Used with prompts based on specific criteria, \textbf{LLMs are currently the best proxy for human evaluation of story generation.}
    \end{itemize}
    \textbf{RQ4: How explainable are the evaluation ratings of {\llm}s?}
\begin{itemize}
    \item \textbf{LLMs understand the ASE task only partially:} notably they struggle to explain their answers with substantiated claims;
    \item \textbf{Pretraining data helps explain LLM performance at ASG:} larger LLMs seem to produce output that is more similar to existing books.
\end{itemize}
\end{frame}

\begin{frame}{Limitations and Future Perspectives}
    \begin{itemize}
        \item Manually annotating stories is an arduous task. We recruited novices, but \textbf{expert annotation may yield different results};
        \item Our different Eval-Prompts were fairly basic: \textbf{more complex Eval-prompts may yield more interesting results}, \eg\ with Chain-of-Thought prompting; 
        \item As the LLM scene is changing ever so rapidly, \textbf{our results may already be partially outdated};
        \item We would have liked to \textbf{design specific ASE measures}, ideally measures that would highly correlate with our criteria;
        \item We believe \textbf{further exploration of LLM explainability is crucially needed};
        \item More generally, we believe \textbf{the societal and environmental risks posed by LLMs should be more broadly discussed}.
    \end{itemize}
\end{frame}

\begin{frame}{Publications}
    \begin{enumerate}
        \item \textbf{Cyril Chhun}, Pierre Colombo, Fabian M. Suchanek, and Chloé Clavel. 2022. Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation. In \textit{Proceedings of the 29th International Conference on Computational Linguistics (COLING)}, pages 5794--5836.\\\url{https://aclanthology.org/2022.coling-1.509/}
        \item \textbf{Cyril Chhun}, Fabian M. Suchanek, Chloé Clavel. 2024. Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation. \textit{Transactions of the Association for Computational Linguistics (TACL)}, volume 12, pages 1122--1142.\\\url{https://doi.org/10.1162/tacl_a_00689}
    \end{enumerate}
\end{frame}

\begin{frame}{Recap Slide. Ready for the Questions!}
    \begin{itemize}
        \item 6 criteria: \textbf{Relevance}, \textbf{Coherence}, \textbf{Empathy}, \textbf{Surprise}, \textbf{Engagement}, and \textbf{Complexity};
        \item \textbf{HANNA corpus}: 1,056 stories annotated by 3 human raters and 4 LLMs + 480 LLM stories annotated by 3 LLMs;
        \item Meta-evaluation benchmark: \textbf{LLMs are currently the best automatic proxy for human judgment}. While system-level correlations are satisfactory, overall correlations remain weak;
        \item Experiments on LLM explainability: \textbf{LLMs often fail to provide explanations} despite being explicitly asked; when they provide one, they are usually specific but \textbf{do not always substantiate their claims}.
        \item Future directions: more refined annotation protocols, specific ASE measures, further investigation of LLM explainability and societal risks.
    \end{itemize}
\end{frame}